# Offline Text2SQL
Run your Text2SQL Models Locally with Ollama


# Locally Hosted Text2SQL App â€“ Works Completely Offline
This is a quick demo of how to get a fully local Text2SQL model up and running using gemma with Ollama and llamaindex.


## Install dependencies
Ollama, an accessible tool for MacOS, Linux, and Windows users, offers seamless installation from its official website https://ollama.com/download.

 just open your terminal & run:
```
$ ollama run gemma
```
Clone this repository and run the below command from this Repository to install the dependencies

```
$ pip install -r requirements.txt
```
A very simple streamlit app that demonstrates how to use LLM chat in a web app.


```
$ streamlit run app.py
```
In terminal you will see a local host link, click on it to open the app in your browser.

example: http://localhost:8501

